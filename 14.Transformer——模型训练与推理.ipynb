{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.Transformer——模型训练与推理\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 能够编写数据加载与预处理代码\n",
    "\n",
    "2. 熟悉Transformer模型的超参数设置\n",
    "\n",
    "3. 能够用代码实现Transformer模型的训练\n",
    "\n",
    "4. 能用代码实现Transformer模型的推理\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 不仅在机器翻译上表现出色，还被广泛应用于其他 NLP 任务，如文本摘要、问答系统、文本分类等。自原始 Transformer 提出以来，出现了许多改进和变体，如 BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pre-trained Transformer）等，这些模型在不同的任务上都取得了显著的成果。\n",
    "\n",
    "在这次课程中，我们将学习如何使用数据集来训练一个Transformer模型，并推理中英文翻译的任务。\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tools import transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512          # 模型维度\n",
    "num_heads = 8          # 多头注意力头数\n",
    "num_layers = 6         # 编码器和解码器层数\n",
    "d_ffn = 2048           # 前馈神经网络隐藏层维度\n",
    "dropout = 0.1          # Dropout 概率\n",
    "max_len = 50           # 最大序列长度\n",
    "batch_size = 32        # 批大小\n",
    "num_epochs = 50        # 训练轮数\n",
    "lr = 1e-4               # 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 自定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len):\n",
    "        \"\"\"\n",
    "        初始化数据集类。\n",
    "\n",
    "        参数:\n",
    "        - src_sentences: 源语言句子列表（例如中文句子）。\n",
    "        - tgt_sentences: 目标语言句子列表（例如英文句子）。\n",
    "        - src_vocab: 源语言词汇表（字典，将单词映射到索引）。\n",
    "        - tgt_vocab: 目标语言词汇表（字典，将单词映射到索引）。\n",
    "        - max_len: 句子的最大长度（超过此长度的句子将被截断，不足的将被填充）。\n",
    "        \"\"\"\n",
    "        self.src_sentences = src_sentences  # 源语言句子列表\n",
    "        self.tgt_sentences = tgt_sentences  # 目标语言句子列表\n",
    "        self.src_vocab = src_vocab  # 源语言词汇表\n",
    "        self.tgt_vocab = tgt_vocab  # 目标语言词汇表\n",
    "        self.max_len = max_len  # 句子的最大长度\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集中句子的总数。\n",
    "        \"\"\"\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据索引返回一个数据样本。\n",
    "\n",
    "        参数:\n",
    "        - idx: 数据样本的索引。\n",
    "\n",
    "        返回:\n",
    "        - src_indices: 源语言句子的索引序列（转换为 PyTorch 张量）。\n",
    "        - tgt_indices: 目标语言句子的索引序列（转换为 PyTorch 张量）。\n",
    "        \"\"\"\n",
    "        src_sentence = self.src_sentences[idx]  # 获取源语言句子\n",
    "        tgt_sentence = self.tgt_sentences[idx]  # 获取目标语言句子\n",
    "\n",
    "        # 将句子转换为索引序列\n",
    "        # 使用词汇表将单词映射为索引，如果单词不在词汇表中，则使用 '<unk>'（未知词）的索引\n",
    "        src_indices = [self.src_vocab.get(word, self.src_vocab['<unk>']) for word in src_sentence.strip().split()]\n",
    "        tgt_indices = [self.tgt_vocab.get(word, self.tgt_vocab['<unk>']) for word in tgt_sentence.strip().split()]\n",
    "\n",
    "        # 填充或截断到固定长度\n",
    "        # 使用 <pad>（填充词）将句子填充到 max_len，或截断超过 max_len 的部分\n",
    "        src_indices = self.pad_or_truncate(src_indices, self.max_len, self.src_vocab['<pad>'])\n",
    "        tgt_indices = self.pad_or_truncate(tgt_indices, self.max_len, self.tgt_vocab['<pad>'])\n",
    "\n",
    "        # 将索引序列转换为 PyTorch 张量，并返回\n",
    "        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(tgt_indices, dtype=torch.long)\n",
    "\n",
    "    def pad_or_truncate(self, sequence, max_len, pad_token):\n",
    "        \"\"\"\n",
    "        将序列填充或截断到固定长度。\n",
    "\n",
    "        参数:\n",
    "        - sequence: 输入的索引序列。\n",
    "        - max_len: 目标长度。\n",
    "        - pad_token: 填充词的索引。\n",
    "\n",
    "        返回:\n",
    "        - 填充或截断后的序列。\n",
    "        \"\"\"\n",
    "        if len(sequence) < max_len:\n",
    "            # 如果序列长度小于 max_len，则在末尾填充 pad_token\n",
    "            sequence = sequence + [pad_token] * (max_len - len(sequence))\n",
    "        else:\n",
    "            # 如果序列长度大于 max_len，则截断到 max_len\n",
    "            sequence = sequence[:max_len]\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, special_tokens=None):\n",
    "    \"\"\"\n",
    "    构建词汇表。\n",
    "\n",
    "    参数:\n",
    "    - sentences: 句子列表，每个句子是一个字符串。\n",
    "    - special_tokens: 特殊符号列表（如 '<pad>', '<unk>', '<sos>', '<eos>'），这些符号会被优先添加到词汇表中。\n",
    "\n",
    "    返回:\n",
    "    - vocab: 词汇表（字典，将单词映射到唯一的索引）。\n",
    "    \"\"\"\n",
    "    # 使用 defaultdict 创建词汇表\n",
    "    # lambda: len(vocab) 表示默认值为当前词汇表的长度（即新词的索引）\n",
    "    vocab = defaultdict(lambda: len(vocab))\n",
    "\n",
    "    # 如果有特殊符号，优先添加到词汇表中\n",
    "    if special_tokens:\n",
    "        for token in special_tokens:\n",
    "            vocab[token]  # 将特殊符号添加到词汇表，索引为当前词汇表长度\n",
    "\n",
    "    # 遍历所有句子，将句子中的单词添加到词汇表\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.strip().split():  # 按空格分割句子为单词\n",
    "            vocab[word]  # 将单词添加到词汇表，索引为当前词汇表长度\n",
    "\n",
    "    # 返回构建好的词汇表\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义加载数据集函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)  # 没有列名，使用默认列名 0 和 1\n",
    "    # 假设列 0 是中文句子，列 1 是英文句子\n",
    "    src_sentences = df[0].tolist()  # 中文句子\n",
    "    tgt_sentences = df[1].tolist()  # 英文句子\n",
    "    return src_sentences, tgt_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "file_path = \"./datasets/WMT-Chinese-to-English-Machine-Translation-newstest/damo_mt_testsets_zh2en_news_wmt18.csv\"\n",
    "src_sentences, tgt_sentences = load_dataset(file_path)  # 调用 load_dataset 函数加载源语言和目标语言句子\n",
    "\n",
    "# 划分训练集和测试集（80% 训练集，20% 测试集）\n",
    "# 使用 train_test_split 函数将数据集划分为训练集和测试集\n",
    "# test_size=0.2 表示测试集占 20%，random_state=42 确保每次划分结果一致\n",
    "src_train, src_test, tgt_train, tgt_test = train_test_split(\n",
    "    src_sentences, tgt_sentences, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 构建词汇表\n",
    "# 定义特殊符号列表，包括填充符、未知词、句子开始符和句子结束符\n",
    "special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "# 使用训练集的源语言句子构建源语言词汇表\n",
    "src_vocab = build_vocab(src_train, special_tokens)  # 仅使用训练集构建词汇表\n",
    "# 使用训练集的目标语言句子构建目标语言词汇表\n",
    "tgt_vocab = build_vocab(tgt_train, special_tokens)  # 仅使用训练集构建词汇表\n",
    "\n",
    "# 创建训练集和测试集\n",
    "# 使用 TranslationDataset 类将训练集和测试集封装为 PyTorch 数据集\n",
    "train_dataset = TranslationDataset(src_train, tgt_train, src_vocab, tgt_vocab, max_len)\n",
    "test_dataset = TranslationDataset(src_test, tgt_test, src_vocab, tgt_vocab, max_len)\n",
    "\n",
    "# 创建 DataLoader\n",
    "# 使用 DataLoader 将数据集封装为可迭代的数据加载器\n",
    "# batch_size 表示每个批次的样本数量，shuffle=True 表示打乱训练集数据\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# shuffle=False 表示不打乱测试集数据\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    训练模型的一个 epoch。\n",
    "\n",
    "    参数:\n",
    "    - model: 要训练的模型。\n",
    "    - dataloader: 数据加载器，提供训练数据批次。\n",
    "    - optimizer: 优化器，用于更新模型参数。\n",
    "    - criterion: 损失函数，用于计算模型输出和目标之间的损失。\n",
    "    - device: 设备（如 'cuda' 或 'cpu'），用于指定模型和数据的计算设备。\n",
    "\n",
    "    返回:\n",
    "    - 平均损失值（一个 epoch 的总损失除以批次数量）。\n",
    "    \"\"\"\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "    total_loss = 0  # 初始化总损失\n",
    "\n",
    "    # 遍历数据加载器中的每个批次\n",
    "    for src, tgt in dataloader:\n",
    "        # 将数据移动到指定设备（如 GPU）\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()  # 清空梯度缓存\n",
    "        # 模型输入：源语言句子和目标语言句子（不包括最后一个词）\n",
    "        output = model(src, tgt[:, :-1])  # 解码器输入不包括最后一个词\n",
    "        # 计算损失：模型输出和目标语言句子（不包括第一个词）之间的交叉熵损失\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()  # 计算梯度\n",
    "        optimizer.step()  # 更新模型参数\n",
    "\n",
    "        # 累加损失\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 返回平均损失值\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    评估模型性能。\n",
    "\n",
    "    参数:\n",
    "    - model: 要评估的模型。\n",
    "    - dataloader: 数据加载器，提供评估数据批次。\n",
    "    - criterion: 损失函数，用于计算模型输出和目标之间的损失。\n",
    "    - device: 设备（如 'cuda' 或 'cpu'），用于指定模型和数据的计算设备。\n",
    "\n",
    "    返回:\n",
    "    - 平均损失值（总损失除以批次数量）。\n",
    "    - BLEU 分数，用于评估翻译质量。\n",
    "    \"\"\"\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    total_loss = 0  # 初始化总损失\n",
    "    all_preds = []  # 保存所有预测的句子索引\n",
    "    all_targets = []  # 保存所有目标句子索引\n",
    "\n",
    "    # 禁用梯度计算，减少内存消耗并加速计算\n",
    "    with torch.no_grad():\n",
    "        # 遍历数据加载器中的每个批次\n",
    "        for src, tgt in dataloader:\n",
    "            # 将数据移动到指定设备（如 GPU）\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            # 模型输入：源语言句子和目标语言句子（不包括最后一个词）\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            # 计算损失：模型输出和目标语言句子（不包括第一个词）之间的交叉熵损失\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()  # 累加损失\n",
    "\n",
    "            # 保存预测和目标句子\n",
    "            # 使用 torch.argmax 获取模型预测的词索引\n",
    "            preds = torch.argmax(output, dim=-1)\n",
    "            # 将预测和目标句子的索引保存到列表中\n",
    "            all_preds.extend(preds.cpu().numpy())  # 将张量转换为 NumPy 数组并保存\n",
    "            all_targets.extend(tgt[:, 1:].cpu().numpy())  # 目标句子去掉第一个词\n",
    "\n",
    "    # 计算 BLEU 分数\n",
    "    # 使用 calculate_bleu 函数计算预测句子和目标句子之间的 BLEU 分数\n",
    "    bleu_score = calculate_bleu(all_preds, all_targets, dataloader.dataset.tgt_vocab)\n",
    "\n",
    "    # 返回平均损失值和 BLEU 分数\n",
    "    return total_loss / len(dataloader), bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 计算 BLEU 分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(preds, targets, tgt_vocab):\n",
    "    \"\"\"\n",
    "    计算预测句子和目标句子之间的 BLEU 分数。\n",
    "\n",
    "    参数:\n",
    "    - preds: 预测句子的索引列表（每个句子是一个索引列表）。\n",
    "    - targets: 目标句子的索引列表（每个句子是一个索引列表）。\n",
    "    - tgt_vocab: 目标语言词汇表（字典，将单词映射到索引）。\n",
    "\n",
    "    返回:\n",
    "    - 平均 BLEU 分数。\n",
    "    \"\"\"\n",
    "    # 将索引转换为单词\n",
    "    # 创建索引到单词的映射字典\n",
    "    idx_to_word = {idx: word for word, idx in tgt_vocab.items()}\n",
    "\n",
    "    # 初始化 BLEU 分数列表\n",
    "    bleu_scores = []\n",
    "\n",
    "    # 使用平滑函数处理 BLEU 分数计算中的零值问题\n",
    "    smoothing = SmoothingFunction().method1\n",
    "\n",
    "    # 遍历预测和目标句子\n",
    "    for pred, target in zip(preds, targets):\n",
    "        # 将预测句子的索引转换为单词，并过滤掉填充符 '<pad>'\n",
    "        pred_sentence = [idx_to_word[idx] for idx in pred if idx != tgt_vocab['<pad>']]\n",
    "        # 将目标句子的索引转换为单词，并过滤掉填充符 '<pad>'\n",
    "        target_sentence = [idx_to_word[idx] for idx in target if idx != tgt_vocab['<pad>']]\n",
    "\n",
    "        # 计算当前句子的 BLEU 分数\n",
    "        # sentence_bleu 计算单个句子的 BLEU 分数\n",
    "        # [target_sentence] 表示将目标句子作为参考句子列表\n",
    "        # smoothing_function 用于处理零值问题\n",
    "        bleu_scores.append(sentence_bleu([target_sentence], pred_sentence, smoothing_function=smoothing))\n",
    "\n",
    "    # 返回所有句子的平均 BLEU 分数\n",
    "    return np.mean(bleu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 定义生成翻译函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation(model, src_sentence, src_vocab, tgt_vocab, max_len, device):\n",
    "    \"\"\"\n",
    "    使用模型生成翻译结果。\n",
    "\n",
    "    参数:\n",
    "    - model: 训练好的翻译模型。\n",
    "    - src_sentence: 源语言句子（字符串）。\n",
    "    - src_vocab: 源语言词汇表（字典，将单词映射到索引）。\n",
    "    - tgt_vocab: 目标语言词汇表（字典，将单词映射到索引）。\n",
    "    - max_len: 生成句子的最大长度。\n",
    "    - device: 设备（如 'cuda' 或 'cpu'），用于指定模型和数据的计算设备。\n",
    "\n",
    "    返回:\n",
    "    - 翻译后的目标语言句子（字符串）。\n",
    "    \"\"\"\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "    # 将源句子转换为索引序列\n",
    "    # 使用源语言词汇表将单词映射为索引，未知词用 '<unk>' 的索引代替\n",
    "    src_indices = [src_vocab.get(word, src_vocab['<unk>']) for word in src_sentence.strip().split()]\n",
    "    # 将索引序列转换为 PyTorch 张量，并添加批次维度（unsqueeze(0)）\n",
    "    src_indices = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # 初始化目标句子（以 <sos> 开始）\n",
    "    tgt_indices = [tgt_vocab['<sos>']]  # 开始符号\n",
    "\n",
    "    # 逐词生成翻译结果\n",
    "    for _ in range(max_len):\n",
    "        # 将目标句子的索引序列转换为 PyTorch 张量，并添加批次维度\n",
    "        tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        # 前向传播：将源句子和当前目标句子输入模型，得到输出\n",
    "        output = model(src_indices, tgt_tensor)\n",
    "        # 获取下一个词的索引（选择概率最大的词）\n",
    "        next_word = torch.argmax(output[:, -1, :], dim=-1).item()\n",
    "\n",
    "        # 检查索引是否有效\n",
    "        if next_word >= len(tgt_vocab):\n",
    "            next_word = tgt_vocab['<unk>']  # 使用未知词索引\n",
    "\n",
    "        # 将下一个词添加到目标句子中\n",
    "        tgt_indices.append(next_word)\n",
    "        # 如果生成的是结束符号 <eos>，则停止生成\n",
    "        if next_word == tgt_vocab['<eos>']:  # 结束符号\n",
    "            break\n",
    "\n",
    "    # 将索引转换为单词\n",
    "    # 创建索引到单词的映射字典\n",
    "    idx_to_word = {idx: word for word, idx in tgt_vocab.items()}\n",
    "    # 将目标句子的索引序列转换为单词列表\n",
    "    translation = [idx_to_word[idx] for idx in tgt_indices]\n",
    "    # 去掉 <sos> 和 <eos>，并将单词列表拼接为字符串\n",
    "    return \" \".join(translation[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 模型训练与推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 7.9980, Val Loss: 7.6718, BLEU: 0.0052\n",
      "Epoch [2/50], Train Loss: 7.3035, Val Loss: 7.3415, BLEU: 0.0098\n",
      "Epoch [3/50], Train Loss: 6.8615, Val Loss: 7.0353, BLEU: 0.0117\n",
      "Epoch [4/50], Train Loss: 6.3539, Val Loss: 6.5641, BLEU: 0.0151\n",
      "Epoch [5/50], Train Loss: 5.8010, Val Loss: 6.1288, BLEU: 0.0204\n",
      "Epoch [6/50], Train Loss: 5.2914, Val Loss: 5.7207, BLEU: 0.0279\n",
      "Epoch [7/50], Train Loss: 4.8419, Val Loss: 5.3858, BLEU: 0.0384\n",
      "Epoch [8/50], Train Loss: 4.4386, Val Loss: 5.0470, BLEU: 0.0505\n",
      "Epoch [9/50], Train Loss: 4.0594, Val Loss: 4.7107, BLEU: 0.0630\n",
      "Epoch [10/50], Train Loss: 3.6928, Val Loss: 4.4772, BLEU: 0.0796\n",
      "Epoch [11/50], Train Loss: 3.3560, Val Loss: 4.2185, BLEU: 0.0971\n",
      "Epoch [12/50], Train Loss: 3.0415, Val Loss: 3.9930, BLEU: 0.1130\n",
      "Epoch [13/50], Train Loss: 2.7447, Val Loss: 3.8052, BLEU: 0.1276\n",
      "Epoch [14/50], Train Loss: 2.4703, Val Loss: 3.5629, BLEU: 0.1437\n",
      "Epoch [15/50], Train Loss: 2.2207, Val Loss: 3.4534, BLEU: 0.1571\n",
      "Epoch [16/50], Train Loss: 1.9800, Val Loss: 3.3342, BLEU: 0.1682\n",
      "Epoch [17/50], Train Loss: 1.7617, Val Loss: 3.1849, BLEU: 0.1796\n",
      "Epoch [18/50], Train Loss: 1.5546, Val Loss: 3.0918, BLEU: 0.1888\n",
      "Epoch [19/50], Train Loss: 1.3687, Val Loss: 2.9881, BLEU: 0.1997\n",
      "Epoch [20/50], Train Loss: 1.1992, Val Loss: 2.8536, BLEU: 0.2095\n",
      "Epoch [21/50], Train Loss: 1.0411, Val Loss: 2.8157, BLEU: 0.2146\n",
      "Epoch [22/50], Train Loss: 0.8961, Val Loss: 2.7743, BLEU: 0.2203\n",
      "Epoch [23/50], Train Loss: 0.7630, Val Loss: 2.6992, BLEU: 0.2261\n",
      "Epoch [24/50], Train Loss: 0.6496, Val Loss: 2.6509, BLEU: 0.2314\n",
      "Epoch [25/50], Train Loss: 0.5462, Val Loss: 2.6243, BLEU: 0.2351\n",
      "Epoch [26/50], Train Loss: 0.4527, Val Loss: 2.5577, BLEU: 0.2388\n",
      "Epoch [27/50], Train Loss: 0.3738, Val Loss: 2.5652, BLEU: 0.2414\n",
      "Epoch [28/50], Train Loss: 0.3131, Val Loss: 2.4888, BLEU: 0.2439\n",
      "Epoch [29/50], Train Loss: 0.2574, Val Loss: 2.4655, BLEU: 0.2468\n",
      "Epoch [30/50], Train Loss: 0.2137, Val Loss: 2.4526, BLEU: 0.2507\n",
      "Epoch [31/50], Train Loss: 0.1804, Val Loss: 2.4402, BLEU: 0.2498\n",
      "Epoch [32/50], Train Loss: 0.1523, Val Loss: 2.4106, BLEU: 0.2526\n",
      "Epoch [33/50], Train Loss: 0.1288, Val Loss: 2.3605, BLEU: 0.2544\n",
      "Epoch [34/50], Train Loss: 0.1122, Val Loss: 2.3890, BLEU: 0.2565\n",
      "Epoch [35/50], Train Loss: 0.0994, Val Loss: 2.3532, BLEU: 0.2557\n",
      "Epoch [36/50], Train Loss: 0.0884, Val Loss: 2.3539, BLEU: 0.2573\n",
      "Epoch [37/50], Train Loss: 0.0771, Val Loss: 2.2938, BLEU: 0.2593\n",
      "Epoch [38/50], Train Loss: 0.0689, Val Loss: 2.3097, BLEU: 0.2582\n",
      "Epoch [39/50], Train Loss: 0.0613, Val Loss: 2.2980, BLEU: 0.2603\n",
      "Epoch [40/50], Train Loss: 0.0579, Val Loss: 2.3279, BLEU: 0.2586\n",
      "Epoch [41/50], Train Loss: 0.0518, Val Loss: 2.2845, BLEU: 0.2624\n",
      "Epoch [42/50], Train Loss: 0.0491, Val Loss: 2.2931, BLEU: 0.2626\n",
      "Epoch [43/50], Train Loss: 0.0465, Val Loss: 2.3118, BLEU: 0.2611\n",
      "Epoch [44/50], Train Loss: 0.0425, Val Loss: 2.2719, BLEU: 0.2614\n",
      "Epoch [45/50], Train Loss: 0.0399, Val Loss: 2.2631, BLEU: 0.2631\n",
      "Epoch [46/50], Train Loss: 0.0379, Val Loss: 2.2473, BLEU: 0.2631\n",
      "Epoch [47/50], Train Loss: 0.0333, Val Loss: 2.2850, BLEU: 0.2650\n",
      "Epoch [48/50], Train Loss: 0.0318, Val Loss: 2.2738, BLEU: 0.2638\n",
      "Epoch [49/50], Train Loss: 0.0287, Val Loss: 2.2460, BLEU: 0.2669\n",
      "Epoch [50/50], Train Loss: 0.0265, Val Loss: 2.2572, BLEU: 0.2659\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型、优化器和损失函数\n",
    "# 创建 Transformer 模型实例\n",
    "# 参数:\n",
    "# - len(src_vocab): 源语言词汇表大小。\n",
    "# - len(tgt_vocab): 目标语言词汇表大小。\n",
    "# - d_model: 模型的维度（隐藏层大小）。\n",
    "# - num_heads: 多头注意力机制的头数。\n",
    "# - num_layers: 编码器和解码器的层数。\n",
    "# - d_ffn: 前馈神经网络的隐藏层维度。\n",
    "# - dropout: Dropout 概率。\n",
    "# - max_len: 句子的最大长度。\n",
    "model = transformer.Transformer(len(src_vocab), len(tgt_vocab), d_model, num_heads, num_layers, d_ffn, dropout, max_len).to(device)\n",
    "\n",
    "# 创建 Adam 优化器\n",
    "# 参数:\n",
    "# - model.parameters(): 模型的可训练参数。\n",
    "# - lr: 学习率。\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# 创建损失函数（交叉熵损失）\n",
    "# 参数:\n",
    "# - ignore_index=src_vocab['<pad>']: 忽略填充符 '<pad>' 的损失计算。\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab['<pad>'])\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练一个 epoch，并返回训练损失\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
    "    \n",
    "    # 在测试集上评估模型，返回验证损失和 BLEU 分数\n",
    "    val_loss, bleu_score = evaluate(model, test_dataloader, criterion, device)\n",
    "    \n",
    "    # 打印当前 epoch 的训练损失、验证损失和 BLEU 分数\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, BLEU: {bleu_score:.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "# 将模型的参数保存到文件 \"transformer_model.pth\" 中\n",
    "torch.save(model.state_dict(), \"transformer_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始推理...\n",
      "生成翻译: heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat heat\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n开始推理...\")\n",
    "\n",
    "# 输入中文句子\n",
    "src_sentence = \"你好，世界！\"\n",
    "\n",
    "# 调用 generate_translation 函数生成翻译结果\n",
    "# 参数:\n",
    "# - model: 训练好的翻译模型。\n",
    "# - src_sentence: 输入的源语言句子（中文句子）。\n",
    "# - src_vocab: 源语言词汇表（字典，将单词映射到索引）。\n",
    "# - tgt_vocab: 目标语言词汇表（字典，将单词映射到索引）。\n",
    "# - max_len: 生成句子的最大长度。\n",
    "# - device: 设备（如 'cuda' 或 'cpu'），用于指定模型和数据的计算设备。\n",
    "translation = generate_translation(model, src_sentence, src_vocab, tgt_vocab, max_len, device)\n",
    "\n",
    "# 打印生成的翻译结果\n",
    "print(f\"生成翻译: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
