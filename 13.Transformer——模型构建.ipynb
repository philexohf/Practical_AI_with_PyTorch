{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.Transformerâ€”â€”æ¨¡å‹æ„å»º\n",
    "\n",
    "**å­¦ä¹ ç›®æ ‡**\n",
    "\n",
    "1. èƒ½ç”¨ä»£ç å®ç°åŸºäºæ­£å¼¦ä½™å¼¦å‡½æ•°çš„ä½ç½®ç¼–ç \n",
    "\n",
    "2. èƒ½ç”¨ä»£ç å®ç°ä½ç½®ç¼–ç çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰\n",
    "\n",
    "3. èƒ½ç”¨ä»£ç å®ç°æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–(Add&Norm)ç»“æ„\n",
    "\n",
    "4. èƒ½ç”¨ä»£ç å®ç°Transformerçš„ç¼–ç å™¨å’Œè§£ç å™¨æ¨¡å—\n",
    "\n",
    "5. èƒ½ç”¨ä»£ç æ„å»ºTransformeræ¨¡å‹\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å†æ¥å›é¡¾ä¸€ä¸‹ä¸Šæ¬¡è¯¾ç¨‹è®²è§£çš„Transformeræ¨¡å‹æ¶æ„ï¼š\n",
    "\n",
    "Transformer æ¨¡å‹ä¸»è¦ç”±ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤éƒ¨åˆ†ç»„æˆï¼Œæ¯éƒ¨åˆ†éƒ½ç”±å¤šä¸ªç›¸åŒçš„å±‚ï¼ˆLayerï¼‰å †å è€Œæˆã€‚æ ‡å‡†çš„ Transformer é€šå¸¸åŒ…æ‹¬ 6 å±‚ç¼–ç å™¨å’Œ 6 å±‚è§£ç å™¨ã€‚ç¼–ç å™¨è´Ÿè´£å°†è¾“å…¥åºåˆ—è½¬åŒ–ä¸ºé«˜ç»´è¡¨ç¤ºï¼Œè§£ç å™¨åˆ™æ ¹æ®è¿™äº›è¡¨ç¤ºç”Ÿæˆè¾“å‡ºåºåˆ—ã€‚\n",
    "\n",
    "<img src=\"./images/transformer.jpg\" style=\"zoom:60%;\" />\n",
    "\n",
    "ï¼ˆ1ï¼‰æ¯ä¸ªç¼–ç å™¨éƒ½åŒ…æ‹¬ä¸¤ä¸ªå­å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚æ¯ä¸ªå­å±‚åéƒ½åº”ç”¨äº†æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ã€‚\n",
    "\n",
    "ï¼ˆ2ï¼‰æ¯ä¸ªè§£ç å™¨åŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªå­å±‚ï¼šæ©è”½å¤šå¤´è‡ªæ³¨æ„åŠ›ã€ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚åŒæ ·ï¼Œæ¯ä¸ªå­å±‚åä¹Ÿåº”ç”¨äº†æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼Œä»¥ä¿ƒè¿›æ¢¯åº¦æµåŠ¨å’Œç¨³å®šè®­ç»ƒã€‚\n",
    "\n",
    "å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ï¼šTransformer é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹åŒæ—¶ä»ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´æ•æ‰ä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚\n",
    "\n",
    "ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼šç”±äº Transformer æ¨¡å‹æœ¬èº«ä¸å…·å¤‡æ•æ‰åºåˆ—é¡ºåºçš„èƒ½åŠ›ï¼Œå› æ­¤å¼•å…¥äº†ä½ç½®ç¼–ç æ¥æä¾›åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„ä½ç½®ä¿¡æ¯ã€‚\n",
    "\n",
    "å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰å’Œæ®‹å·®è¿æ¥ï¼ˆResidual Connectionsï¼‰ï¼šTransformer ä½¿ç”¨å±‚å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥æ¥ä¿ƒè¿›æ·±å±‚ç½‘ç»œçš„è®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸é—®é¢˜ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†èƒ½å¤Ÿé¡ºåˆ©æ„å»ºå‡ºTransformeræ¨¡å‹ï¼Œä¸‹é¢æˆ‘ä»¬é€ä¸€æ„å»ºTransformerçš„å‡ ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š\n",
    "\n",
    "- ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰\n",
    "- ä½ç½®ç¼–ç çš„å‰é¦ˆç½‘ç»œï¼ˆFeed Forward Networkï¼‰\n",
    "- æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–(Add&Norm)\n",
    "- Transformerç¼–ç å™¨ï¼ˆEncoderï¼‰\n",
    "- Transformerè§£ç å™¨ï¼ˆDecoderï¼‰\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tools import transformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰è¶…å‚æ•°\n",
    "vocab_size = 100256  # GPT4çš„è¯åº“å¤§å°\n",
    "d_model = 512          # æ¨¡å‹ç»´åº¦\n",
    "num_heads = 8          # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°\n",
    "num_layers = 6         # ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°\n",
    "d_ffn = 2048            # å‰é¦ˆç¥ç»ç½‘ç»œéšè—å±‚ç»´åº¦\n",
    "dropout = 0.1          # Dropout æ¦‚ç‡\n",
    "max_len = 1000          # æœ€å¤§åºåˆ—é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [[42016, 78228, 80578, 0, 37046, 164, 116, 102, 84949, 222, 30590, 97565, 35287], [37046, 47551, 19000, 56386, 61056, 97565, 1811]]\n",
      "Decoded Texts: ['åŒå¿—ä»¬!æˆ‘è¸©ç€åœ°é›·äº†', 'æˆ‘ç°åœ¨å¼€å§‹æ’é›·ã€‚']\n",
      "Token IDs:\n",
      " tensor([[42016, 78228, 80578,     0, 37046,   164,   116,   102, 84949,   222,\n",
      "         30590, 97565, 35287],\n",
      "        [37046, 47551, 19000, 56386, 61056, 97565,  1811,     0,     0,     0,\n",
      "             0,     0,     0]], device='cuda:0')\n",
      "è¯åµŒå…¥å‘é‡:\n",
      " torch.Size([2, 13, 512])\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ tiktoken åˆ†è¯å™¨\n",
    "tiktok_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "torch.manual_seed(42)  # å›ºå®šéšæœºç§å­\n",
    "\n",
    "texts = [\"åŒå¿—ä»¬!æˆ‘è¸©ç€åœ°é›·äº†\", \"æˆ‘ç°åœ¨å¼€å§‹æ’é›·ã€‚\"]\n",
    "\n",
    "# å°†æ–‡æœ¬ç¼–ç ä¸º Token IDs\n",
    "tokens = [tiktok_encoder.encode(text) for text in texts]\n",
    "print(\"Token IDs:\", tokens)\n",
    "\n",
    "# å°† Token IDs è§£ç å›æ–‡æœ¬\n",
    "decoded_texts = [tiktok_encoder.decode(token_ids) for token_ids in tokens]\n",
    "print(\"Decoded Texts:\", decoded_texts)\n",
    "\n",
    "# åˆ›å»ºåµŒå…¥å±‚\n",
    "embedding_layer = nn.Embedding(vocab_size, d_model).to(device)\n",
    "\n",
    "# å°† Token IDs è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶å¡«å……åˆ°ç›¸åŒé•¿åº¦\n",
    "token_ids = [torch.tensor(token_ids) for token_ids in tokens]\n",
    "token_ids = torch.nn.utils.rnn.pad_sequence(token_ids, batch_first=True).to(device)  # å¡«å……åˆ°ç›¸åŒé•¿åº¦\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "\n",
    "# è·å– Token Embedding\n",
    "token_embedding = embedding_layer(token_ids)  # å½¢çŠ¶: [batch_size, seq_len, embedding_dim]\n",
    "print(\"è¯åµŒå…¥å‘é‡:\\n\", token_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1  ä½ç½®ç¼–ç \n",
    "\n",
    "ç”±äºTransformeræ¨¡å‹æœ¬èº«ä¸å…·æœ‰å¤„ç†åºåˆ—é¡ºåºçš„èƒ½åŠ›ï¼Œä½ç½®ç¼–ç ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç†è§£å•è¯åœ¨å¥å­ä¸­çš„ç›¸å¯¹ä½ç½®ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ç”Ÿæˆå”¯ä¸€çš„ç¼–ç ã€‚ä½ç½®ç¼–ç çš„å…¬å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "ğ‘ƒğ¸æ˜¯ä½ç½®ç¼–ç çŸ©é˜µï¼›\n",
    "\n",
    "posæ˜¯è¯åœ¨åºåˆ—ä¸­çš„ç»å¯¹ä½ç½®ï¼ˆä»0å¼€å§‹ï¼‰ï¼›\n",
    "\n",
    "ğ‘–æ˜¯ç»´åº¦ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰ï¼›\n",
    "\n",
    "$ğ‘‘_{\\text{model}}$æ˜¯æ¨¡å‹çš„ç»´åº¦å¤§å°ã€‚\n",
    "\n",
    "å¯¹äºæ¯ä¸ªç»´åº¦ğ‘–ï¼Œä½ç½®ç¼–ç åŒ…å«ä¸¤ä¸ªå€¼ï¼šä¸€ä¸ªæ­£å¼¦å€¼å’Œä¸€ä¸ªä½™å¼¦å€¼ï¼Œåˆ†åˆ«å¯¹åº”å¶æ•°ç´¢å¼•å’Œå¥‡æ•°ç´¢å¼•ã€‚\n",
    "\n",
    "ä½ç½®ç¼–ç çš„ç›®çš„æ˜¯ç»™æ¨¡å‹æä¾›æ¯ä¸ªè¯åœ¨åºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œè¿™æ ·æ¨¡å‹å°±å¯ä»¥åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥ç†è§£è¯ä¸è¯ä¹‹é—´çš„å…³ç³»ã€‚ä½ç½®ç¼–ç é€šå¸¸è¢«æ·»åŠ åˆ°è¯åµŒå…¥ï¼ˆWord Embeddingsï¼‰ä¸­ï¼Œç„¶åä¸€èµ·è¾“å…¥åˆ°Transformeræ¨¡å‹ä¸­ã€‚\n",
    "\n",
    "Transformerçš„ä½ç½®ç¼–ç é€‰æ‹©ä¸‰è§’å‡½æ•°çš„å®˜æ–¹è§£é‡Šæ˜¯ï¼š\n",
    "\n",
    "ä½ç½®ç¼–ç çš„æ¯ä¸ªç»´åº¦éƒ½å¯¹åº”äºä¸€ä¸ªæ­£å¼¦æ›²çº¿ã€‚æ³¢é•¿å½¢æˆä¸€ä¸ªä»2Ï€åˆ°10000Â·2Ï€çš„å‡ ä½•è½¨è¿¹ã€‚æˆ‘ä»¬ä¹‹æ‰€ä»¥é€‰æ‹©è¿™ä¸ªå‡½æ•°ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬å‡è®¾å®ƒå¯ä»¥è®©æ¨¡å‹å¾ˆå®¹æ˜“åœ°é€šè¿‡ç›¸å¯¹ä½ç½®è¿›è¡Œå­¦ä¹ ï¼Œå› ä¸ºå¯¹äºä»»ä½•å›ºå®šçš„åç§»é‡k,PEpos+kéƒ½å¯ä»¥è¡¨ç¤ºä¸ºPEposçš„çº¿æ€§å‡½æ•°ã€‚\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªç»´åº¦éƒ½æ˜¯æ³¢é•¿ä¸åŒçš„æ­£å¼¦æ³¢ï¼Œæ³¢é•¿èŒƒå›´æ˜¯2Ï€åˆ°10000Â·2Ï€ï¼Œé€‰ç”¨10000è¿™ä¸ªæ¯”è¾ƒå¤§çš„æ•°æ˜¯å› ä¸ºä¸‰è§’å‡½æ•°å¼æœ‰å‘¨æœŸçš„ï¼Œåœ¨è¿™ä¸ªèŒƒå›´åŸºæœ¬ä¸Šï¼Œå°±ä¸ä¼šå‡ºç°æ³¢é•¿ä¸€æ ·çš„æƒ…å†µäº†ã€‚ç„¶åè°·æ­Œçš„ç§‘å­¦å®¶ä»¬ä¸ºäº†è®©PEå€¼çš„å‘¨æœŸæ›´é•¿ï¼Œè¿˜äº¤æ›¿ä½¿ç”¨sinã€cosæ¥è®¡ç®—PEçš„å€¼ï¼Œå°±å¾—åˆ°äº†æœ€ç»ˆçš„å…¬å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ä½ç½®ç¼–ç æ¨¡å—ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "        - d_model: æ¨¡å‹çš„ç»´åº¦ã€‚\n",
    "        - max_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸º 1000ã€‚\n",
    "        - dropout: Dropout çš„æ¦‚ç‡ï¼Œé»˜è®¤ä¸º 0.1ã€‚\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # åˆå§‹åŒ–ä½ç½®ç¼–ç çŸ©é˜µ\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # å¢åŠ  batch ç»´åº¦\n",
    "        self.register_buffer('pe', pe)  # å°†ä½ç½®ç¼–ç æ³¨å†Œä¸ºç¼“å†²åŒº\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "        - X: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "        - è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)ã€‚\n",
    "        \"\"\"\n",
    "        X = X + self.pe[:, :X.size(1), :]  # å°†ä½ç½®ç¼–ç åŠ åˆ°è¾“å…¥ä¸Š\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥åµŒå…¥å‘é‡:\n",
      " tensor([[[-0.0699,  1.3706,  1.4708,  ...,  0.0000, -1.6456,  0.3474],\n",
      "         [-0.5197, -1.2748,  0.8772,  ...,  1.5548,  1.5875,  0.1868],\n",
      "         [ 0.9168, -2.0793, -0.2925,  ...,  1.2703, -0.9609,  0.6663],\n",
      "         ...,\n",
      "         [ 0.0181, -0.6386, -1.3335,  ...,  0.4053, -0.3721,  0.1877],\n",
      "         [-0.0000,  0.0208, -2.1964,  ...,  1.7799,  0.1963,  0.6811],\n",
      "         [ 0.0543, -0.0889, -0.6966,  ...,  2.0988,  0.2045,  2.0233]],\n",
      "\n",
      "        [[-0.0000,  1.9591,  1.3698,  ...,  0.5399, -0.3399,  2.6830],\n",
      "         [ 1.8715,  0.6171,  2.3728,  ...,  0.6622,  1.8515,  0.9824],\n",
      "         [ 1.1599, -0.2656,  0.0497,  ...,  0.8904,  0.0000,  0.0144],\n",
      "         ...,\n",
      "         [ 1.5365,  0.7202,  0.0000,  ...,  1.5813, -0.3754,  1.6866],\n",
      "         [ 1.0299,  1.6575, -0.0293,  ...,  1.5813, -0.3753,  1.6866],\n",
      "         [ 1.5448,  2.5902,  0.0716,  ...,  0.0000, -0.3752,  1.6866]]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n",
      "\n",
      "è¾“å…¥åµŒå…¥å‘é‡çš„å½¢çŠ¶:\n",
      " torch.Size([2, 13, 512])\n"
     ]
    }
   ],
   "source": [
    "pos_encoder = PositionalEncoding(d_model=d_model).to(device)\n",
    "# ä¸º Token Embedding æ·»åŠ ä½ç½®ç¼–ç \n",
    "input_embedding = pos_encoder(token_embedding)  # å½¢çŠ¶: [batch_size, seq_len, embedding_dim]\n",
    "print(\"è¾“å…¥åµŒå…¥å‘é‡:\\n\", input_embedding)\n",
    "print(\"\\nè¾“å…¥åµŒå…¥å‘é‡çš„å½¢çŠ¶:\\n\", input_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤šå¤´è‡ªæ³¨æ„åŠ›å½¢çŠ¶ï¼štorch.Size([2, 13, 512])\n"
     ]
    }
   ],
   "source": [
    "# å®ä¾‹åŒ–å¤šå¤´è‡ªæ³¨æ„åŠ›\n",
    "attention = transformer.MultiHeadSelfAttention(d_model, num_heads, dropout=0.5).to(device)\n",
    "attention.eval()\n",
    "# æ‰“å°å¤šå¤´è‡ªæ³¨æ„åŠ›è¾“å‡ºçš„å½¢çŠ¶\n",
    "mul_att_shape = attention(input_embedding).shape\n",
    "print(f'å¤šå¤´è‡ªæ³¨æ„åŠ›å½¢çŠ¶ï¼š{mul_att_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2  åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰\n",
    "\n",
    "è¿™å°±æ˜¯ä¸€ä¸ªç®€å•çš„MLPï¼Œå®ƒç”±ä¸¤ä¸ªå…¨è¿æ¥å±‚ç»„æˆã€‚è¿™ä¸¤ä¸ªå…¨è¿æ¥å±‚çš„è¾“å…¥æ˜¯æ¥è‡ªå‰ä¸€å±‚çš„è¾“å‡ºå’Œæ¥è‡ªä½ç½®ç¼–ç çš„å‘é‡ã€‚ä½ç½®ç¼–ç å‘é‡æ˜¯ä¸€ç§æ˜ å°„ï¼Œå®ƒå°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°è¾“å…¥å‘é‡ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°è¾“å…¥åºåˆ—ä¸­å„ä¸ªä½ç½®ä¹‹é—´çš„å…³ç³»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ffn, dropout=0.1):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3  æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–(Add&Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼ˆ1ï¼‰ä»€ä¹ˆæ˜¯å±‚è§„èŒƒåŒ–ï¼Ÿ\n",
    "\n",
    "å±‚è§„èŒƒåŒ–ï¼ˆLayer Normalization, LayerNormï¼‰æ˜¯ä¸€ç§ç”¨äºç¥ç»ç½‘ç»œçš„å½’ä¸€åŒ–æŠ€æœ¯ï¼Œä¸»è¦ç”¨äºç¨³å®šç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ã€‚å®ƒä¸æ‰¹é‡è§„èŒƒåŒ–ï¼ˆBatch Normalization, BatchNormï¼‰ç±»ä¼¼ï¼Œä½†é€‚ç”¨çš„åœºæ™¯å’Œå®ç°æ–¹å¼æœ‰æ‰€ä¸åŒã€‚å±‚è§„èŒƒåŒ–æ˜¯å¯¹è¾“å…¥å¼ é‡çš„**æ¯ä¸ªæ ·æœ¬**è¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªæ‰¹æ¬¡è¿›è¡Œå½’ä¸€åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒå¯¹è¾“å…¥å¼ é‡çš„**æœ€åä¸€ä¸ªç»´åº¦**ï¼ˆé€šå¸¸æ˜¯ç‰¹å¾ç»´åº¦ï¼‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å¾—æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾åˆ†å¸ƒæ›´åŠ ç¨³å®šã€‚\n",
    "\n",
    "ï¼ˆ2ï¼‰å±‚è§„èŒƒåŒ–çš„è®¡ç®—å…¬å¼\n",
    "\n",
    "å¯¹äºè¾“å…¥å¼ é‡xï¼Œå±‚è§„èŒƒåŒ–çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\left( \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\right) + \\beta\n",
    "$$\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "-  x ï¼šè¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º  (N, *) ï¼Œå…¶ä¸­  N  æ˜¯æ ·æœ¬æ•°ï¼Œ*è¡¨ç¤ºä»»æ„å…¶ä»–ç»´åº¦ã€‚\n",
    "\n",
    "-  $\\mu$ï¼šè¾“å…¥å¼ é‡åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šçš„å‡å€¼ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š\n",
    "  $$\n",
    "  \\mu = \\frac{1}{H} \\sum_{i=1}^H x_i\n",
    "  $$\n",
    "  å…¶ä¸­  H  æ˜¯æœ€åä¸€ä¸ªç»´åº¦çš„å¤§å°ã€‚\n",
    "\n",
    "- $\\sigma^2$ ï¼šè¾“å…¥å¼ é‡åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šçš„æ–¹å·®ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š\n",
    "  $$\n",
    "  \\sigma^2 = \\frac{1}{H} \\sum_{i=1}^H (x_i - \\mu)^2\n",
    "  $$\n",
    "  \n",
    "\n",
    "- $\\epsilon$ï¼šä¸€ä¸ªå¾ˆå°çš„å¸¸æ•°ï¼Œç”¨äºé˜²æ­¢åˆ†æ¯ä¸ºé›¶ã€‚\n",
    "\n",
    "- $\\gamma$  å’Œ $\\beta$ ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾å› å­å’Œåç§»å› å­ï¼Œå½¢çŠ¶ä¸è¾“å…¥å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦ç›¸åŒã€‚\n",
    "\n",
    "ï¼ˆ3ï¼‰å±‚è§„èŒƒåŒ–ä¸æ‰¹é‡è§„èŒƒåŒ–çš„å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§           | æ‰¹é‡è§„èŒƒåŒ–ï¼ˆBatchNormï¼‰          | å±‚è§„èŒƒåŒ–ï¼ˆLayerNormï¼‰                |\n",
    "| -------------- | -------------------------------- | ------------------------------------ |\n",
    "| **å½’ä¸€åŒ–ç»´åº¦** | å¯¹æ‰¹æ¬¡ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–             | å¯¹æ¯ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œå½’ä¸€åŒ–   |\n",
    "| **é€‚ç”¨åœºæ™¯**   | é€‚ç”¨äºå›ºå®šå¤§å°çš„æ‰¹æ¬¡å’Œå›¾åƒæ•°æ®   | é€‚ç”¨äºå˜é•¿åºåˆ—å’Œå°æ‰¹æ¬¡æ•°æ®           |\n",
    "| **è®¡ç®—æ–¹å¼**   | ä¾èµ–æ‰¹æ¬¡çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰ | ä¾èµ–æ¯ä¸ªæ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰ |\n",
    "| **å¯å­¦ä¹ å‚æ•°** | æœ‰å¯å­¦ä¹ çš„ç¼©æ”¾å› å­å’Œåç§»å› å­     | æœ‰å¯å­¦ä¹ çš„ç¼©æ”¾å› å­å’Œåç§»å› å­         |\n",
    "| **æ€§èƒ½**       | åœ¨æ‰¹æ¬¡è¾ƒå¤§æ—¶æ•ˆæœå¥½               | åœ¨æ‰¹æ¬¡è¾ƒå°æ—¶æ•ˆæœå¥½                   |\n",
    "\n",
    "ï¼ˆ4ï¼‰å±‚è§„èŒƒåŒ–çš„è®¡ç®—ç¤ºä¾‹\n",
    "\n",
    "å‡è®¾è¾“å…¥å¼ é‡  x  çš„å½¢çŠ¶ä¸º  (3, 4) ï¼Œè¡¨ç¤º 3 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 4 ä¸ªç‰¹å¾ã€‚\n",
    "\n",
    "#### è¾“å…¥å¼ é‡\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "2 & 3 & 4 & 5 \\\\\n",
    "3 & 4 & 5 & 6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### è®¡ç®—å‡å€¼å’Œæ–¹å·®\n",
    "\n",
    "- å¯¹æ¯ä¸ªæ ·æœ¬è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼š\n",
    "\n",
    "  - æ ·æœ¬ 1ï¼š\n",
    "\n",
    "    å‡å€¼ \n",
    "    $$\n",
    "    \\mu_1 = \\frac{1+2+3+4}{4} = 2.5\n",
    "    $$\n",
    "    æ–¹å·® \n",
    "    $$\n",
    "    \\sigma_1^2 = \\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4} = 1.25\n",
    "    $$\n",
    "     \n",
    "\n",
    "  - æ ·æœ¬ 2ï¼š\n",
    "\n",
    "    å‡å€¼ \n",
    "    $$\n",
    "    \\mu_2 = \\frac{2+3+4+5}{4} = 3.5\n",
    "    $$\n",
    "    æ–¹å·® \n",
    "    $$\n",
    "    \\sigma_2^2 = \\frac{(2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2}{4} = 1.25\n",
    "    $$\n",
    "     \n",
    "\n",
    "  - æ ·æœ¬ 3ï¼š\n",
    "\n",
    "    å‡å€¼ \n",
    "    $$\n",
    "    \\mu_3 = \\frac{3+4+5+6}{4} = 4.5\n",
    "    $$\n",
    "     æ–¹å·® \n",
    "    $$\n",
    "    sigma_3^2 = \\frac{(3-4.5)^2 + (4-4.5)^2 + (5-4.5)^2 + (6-4.5)^2}{4} = 1.25\n",
    "    $$\n",
    "     \n",
    "\n",
    " å±‚è§„èŒƒåŒ–çš„è®¡ç®—ç»“æœï¼š\n",
    "\n",
    "  $$\n",
    "  \\text{LayerNorm}(x) = \\begin{bmatrix}\n",
    "  \\frac{1-2.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{2-2.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{3-2.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{4-2.5}{\\sqrt{1.25 + \\epsilon}} \\\\\n",
    "  \\frac{2-3.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{3-3.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{4-3.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{5-3.5}{\\sqrt{1.25 + \\epsilon}} \\\\\n",
    "  \\frac{3-4.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{4-4.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{5-4.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{6-4.5}{\\sqrt{1.25 + \\epsilon}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–(Add&Norm)\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– AddNorm æ¨¡å—ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "        - normalized_shape: å±‚è§„èŒƒåŒ–çš„è¾“å…¥å½¢çŠ¶ï¼ˆé€šå¸¸æ˜¯è¾“å…¥å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦ï¼‰ã€‚\n",
    "        - dropout: Dropout çš„æ¦‚ç‡ï¼ˆç”¨äºæ­£åˆ™åŒ–ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­å‡½æ•°ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "        - X: è¾“å…¥å¼ é‡ï¼ˆé€šå¸¸æ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚\n",
    "        - Y: éœ€è¦ä¸ X è¿›è¡Œæ®‹å·®è¿æ¥çš„å¼ é‡ï¼ˆé€šå¸¸æ˜¯å½“å‰å±‚çš„è¾“å‡ºï¼‰ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "        - ç»è¿‡æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–åçš„è¾“å‡ºå¼ é‡ã€‚\n",
    "        \"\"\"\n",
    "        # å¯¹ Y åº”ç”¨ Dropoutï¼Œç„¶åä¸ X è¿›è¡Œæ®‹å·®è¿æ¥ï¼Œæœ€åè¿›è¡Œå±‚è§„èŒƒåŒ–\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯¥æ¨¡å—å®ç°äº† æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ å’Œ å±‚è§„èŒƒåŒ–ï¼ˆLayerNormï¼‰ çš„åŠŸèƒ½ã€‚\n",
    "\n",
    "æ®‹å·®è¿æ¥é€šè¿‡å°†å½“å‰å±‚çš„è¾“å‡ºä¸ä¸Šä¸€å±‚çš„è¾“å‡ºç›¸åŠ ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°è®­ç»ƒæ·±å±‚ç½‘ç»œã€‚\n",
    "\n",
    "å±‚è§„èŒƒåŒ–å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å¾—æ¨¡å‹å¯¹è¾“å…¥çš„åˆ†å¸ƒæ›´åŠ ç¨³å®šã€‚\n",
    "\n",
    "Dropout ç”¨äºæ­£åˆ™åŒ–ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4  ç¼–ç å™¨-è§£ç å™¨ç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ç¼–ç å™¨\n",
    "\n",
    "ç»“åˆä¸Šè¿°ç¼–å†™çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ã€ä½ç½®ç¼–ç æ¨¡å—ã€å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥ç­‰æ¨¡å—ï¼Œå¯ä»¥æ„å»ºç¼–ç å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ffn, dropout):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "        - d_model: æ¨¡å‹çš„ç»´åº¦ï¼ˆæ¯ä¸ªè¯å‘é‡çš„ç»´åº¦ï¼‰ã€‚\n",
    "        - num_heads: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¤´æ•°ã€‚\n",
    "        - d_ffn: å‰é¦ˆç¥ç»ç½‘ç»œä¸­éšè—å±‚çš„ç»´åº¦ã€‚\n",
    "        - dropout: Dropout çš„æ¦‚ç‡ã€‚\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚\n",
    "        self.attention = transformer.MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # å‰é¦ˆç¥ç»ç½‘ç»œå±‚\n",
    "        self.ffn = PositionWiseFFN(d_model, d_ffn, dropout)\n",
    "\n",
    "        # ä¸¤ä¸ª LayerNorm å±‚ï¼Œç”¨äºå½’ä¸€åŒ–\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # ç”¨äºè‡ªæ³¨æ„åŠ›å±‚çš„è¾“å‡º\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # ç”¨äºå‰é¦ˆç¥ç»ç½‘ç»œçš„è¾“å‡º\n",
    "\n",
    "        # Dropout å±‚ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "        - X: ç¼–ç å™¨çš„è¾“å…¥ï¼Œå½¢çŠ¶ä¸º (batch_size, src_seq_len, d_model)ã€‚\n",
    "        - valid_lens: æœ‰æ•ˆåºåˆ—é•¿åº¦ï¼Œç”¨äºå±è”½å¡«å……éƒ¨åˆ†ï¼Œå½¢çŠ¶ä¸º (batch_size,)ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "        - X: ç¼–ç å™¨çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (batch_size, src_seq_len, d_model)ã€‚\n",
    "        \"\"\"\n",
    "        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "        # è¾“å…¥: X (æºè¯­è¨€åºåˆ—)\n",
    "        # ä½¿ç”¨ valid_lens å±è”½å¡«å……éƒ¨åˆ†\n",
    "        attention_output = self.attention(X, valid_lens)  # (batch_size, src_seq_len, d_model)\n",
    "        # æ®‹å·®è¿æ¥ + Dropout\n",
    "        X = X + self.dropout(attention_output)  # (batch_size, src_seq_len, d_model)\n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        X = self.norm1(X)  # (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # 2. å‰é¦ˆç¥ç»ç½‘ç»œ\n",
    "        # è¾“å…¥: X (è‡ªæ³¨æ„åŠ›è¾“å‡º)\n",
    "        ffn_output = self.ffn(X)  # (batch_size, src_seq_len, d_model)\n",
    "        # æ®‹å·®è¿æ¥ + Dropout\n",
    "        X = X + self.dropout(ffn_output)  # (batch_size, src_seq_len, d_model)\n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        X = self.norm2(X)  # (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # è¿”å›ç¼–ç å™¨çš„è¾“å‡º\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 13, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderBlock(d_model, num_heads, d_ffn, dropout).to(device)\n",
    "\n",
    "encoder_outputs = encoder(input_embedding)\n",
    "print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. è§£ç å™¨\n",
    "\n",
    "åŒæ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä»¥ä¸Šç»„ä»¶æ¥æ„å»ºä¸€ä¸ªtransformerè§£ç å™¨ã€‚ç¼–ç å™¨çš„è¾“å‡ºæ˜¯è§£ç å™¨çš„è¾“å…¥ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ffn, dropout):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "        - d_model: æ¨¡å‹çš„ç»´åº¦ï¼ˆæ¯ä¸ªè¯å‘é‡çš„ç»´åº¦ï¼‰ã€‚\n",
    "        - num_heads: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¤´æ•°ã€‚\n",
    "        - d_ffn: å‰é¦ˆç¥ç»ç½‘ç»œä¸­éšè—å±‚çš„ç»´åº¦ã€‚\n",
    "        - dropout: Dropout çš„æ¦‚ç‡ã€‚\n",
    "        \"\"\"\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # ç¬¬ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼ˆç”¨äºè§£ç å™¨è‡ªæ³¨æ„åŠ›ï¼‰\n",
    "        self.attention1 = transformer.MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # ç¬¬äºŒä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼ˆç”¨äºç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼‰\n",
    "        self.attention2 = transformer.MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # å‰é¦ˆç¥ç»ç½‘ç»œå±‚\n",
    "        self.ffn = PositionWiseFFN(d_model, d_ffn, dropout)\n",
    "\n",
    "        # ä¸‰ä¸ª LayerNorm å±‚ï¼Œç”¨äºå½’ä¸€åŒ–\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # ç”¨äºç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å±‚çš„è¾“å‡º\n",
    "        self.norm2 = nn.LayerNorm(d_model)  # ç”¨äºç¬¬äºŒä¸ªæ³¨æ„åŠ›å±‚çš„è¾“å‡º\n",
    "        self.norm3 = nn.LayerNorm(d_model)  # ç”¨äºå‰é¦ˆç¥ç»ç½‘ç»œçš„è¾“å‡º\n",
    "\n",
    "        # Dropout å±‚ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "        - X: è§£ç å™¨çš„è¾“å…¥ï¼Œå½¢çŠ¶ä¸º (batch_size, tgt_seq_len, d_model)ã€‚\n",
    "        - encoder_output: ç¼–ç å™¨çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (batch_size, src_seq_len, d_model)ã€‚\n",
    "        - src_mask: æºè¯­è¨€çš„æ©ç ï¼Œç”¨äºå±è”½å¡«å……éƒ¨åˆ†ï¼Œå½¢çŠ¶ä¸º (batch_size, src_seq_len)ã€‚\n",
    "        - tgt_mask: ç›®æ ‡è¯­è¨€çš„æ©ç ï¼Œç”¨äºå±è”½æœªæ¥ä¿¡æ¯ï¼Œå½¢çŠ¶ä¸º (batch_size, tgt_seq_len)ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "        - X: è§£ç å™¨çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (batch_size, tgt_seq_len, d_model)ã€‚\n",
    "        \"\"\"\n",
    "        # 1. è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆè§£ç å™¨è‡ªæ³¨æ„åŠ›ï¼‰\n",
    "        # è¾“å…¥: X (ç›®æ ‡è¯­è¨€åºåˆ—)\n",
    "        # ä½¿ç”¨ tgt_mask å±è”½æœªæ¥ä¿¡æ¯\n",
    "        attention_output = self.attention1(X, tgt_mask)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # æ®‹å·®è¿æ¥ + Dropout\n",
    "        X = X + self.dropout(attention_output)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        X = self.norm1(X)  # (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # 2. ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶\n",
    "        # è¾“å…¥: X (è§£ç å™¨è‡ªæ³¨æ„åŠ›è¾“å‡º) å’Œ encoder_output (ç¼–ç å™¨è¾“å‡º)\n",
    "        # ä½¿ç”¨ src_mask å±è”½æºè¯­è¨€ä¸­çš„å¡«å……éƒ¨åˆ†\n",
    "        attention_output = self.attention2(X, src_mask)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # æ®‹å·®è¿æ¥ + Dropout\n",
    "        X = X + self.dropout(attention_output)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        X = self.norm2(X)  # (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # 3. å‰é¦ˆç¥ç»ç½‘ç»œ\n",
    "        # è¾“å…¥: X (ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›è¾“å‡º)\n",
    "        ffn_output = self.ffn(X)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # æ®‹å·®è¿æ¥ + Dropout\n",
    "        X = X + self.dropout(ffn_output)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # å±‚å½’ä¸€åŒ–\n",
    "        X = self.norm3(X)  # (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # è¿”å›è§£ç å™¨çš„è¾“å‡º\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 13, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder = DecoderBlock(d_model, num_heads, d_ffn, dropout).to(device)\n",
    "\n",
    "outputs = decoder(encoder_outputs, input_embedding)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5 Transformeræ¨¡å‹æ„å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ffn, dropout, max_len=1000):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "        - src_vocab_size: æºè¯­è¨€è¯æ±‡è¡¨çš„å¤§å°ã€‚\n",
    "        - tgt_vocab_size: ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨çš„å¤§å°ã€‚\n",
    "        - d_model: æ¨¡å‹çš„ç»´åº¦ï¼ˆæ¯ä¸ªè¯å‘é‡çš„ç»´åº¦ï¼‰ã€‚\n",
    "        - num_heads: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å¤´æ•°ã€‚\n",
    "        - num_layers: ç¼–ç å™¨å’Œè§£ç å™¨çš„å±‚æ•°ã€‚\n",
    "        - d_ffn: å‰é¦ˆç¥ç»ç½‘ç»œä¸­éšè—å±‚çš„ç»´åº¦ã€‚\n",
    "        - dropout: Dropout çš„æ¦‚ç‡ã€‚\n",
    "        - max_len: åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ä¸º 1000ã€‚\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # ç¼–ç å™¨çš„è¯åµŒå…¥å±‚ï¼Œå°†æºè¯­è¨€çš„è¯ç´¢å¼•æ˜ å°„ä¸º d_model ç»´çš„å‘é‡\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "\n",
    "        # è§£ç å™¨çš„è¯åµŒå…¥å±‚ï¼Œå°†ç›®æ ‡è¯­è¨€çš„è¯ç´¢å¼•æ˜ å°„ä¸º d_model ç»´çš„å‘é‡\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # ä½ç½®ç¼–ç æ¨¡å—ï¼Œä¸ºè¾“å…¥åºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # ç¼–ç å™¨å±‚ï¼Œç”±å¤šä¸ª EncoderBlock ç»„æˆ\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderBlock(d_model, num_heads, d_ffn, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # è§£ç å™¨å±‚ï¼Œç”±å¤šä¸ª DecoderBlock ç»„æˆ\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, d_ffn, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # çº¿æ€§å±‚ï¼Œå°†è§£ç å™¨çš„è¾“å‡ºæ˜ å°„ä¸ºç›®æ ‡è¯­è¨€è¯æ±‡è¡¨çš„å¤§å°\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        # Dropout å±‚ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        å‚æ•°:\n",
    "        - src: æºè¯­è¨€è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (batch_size, src_seq_len)ã€‚\n",
    "        - tgt: ç›®æ ‡è¯­è¨€è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (batch_size, tgt_seq_len)ã€‚\n",
    "        - src_mask: æºè¯­è¨€çš„æ©ç ï¼Œç”¨äºå±è”½å¡«å……éƒ¨åˆ†ï¼Œå½¢çŠ¶ä¸º (batch_size, src_seq_len)ã€‚\n",
    "        - tgt_mask: ç›®æ ‡è¯­è¨€çš„æ©ç ï¼Œç”¨äºå±è”½æœªæ¥ä¿¡æ¯ï¼Œå½¢çŠ¶ä¸º (batch_size, tgt_seq_len)ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "        - output: æ¨¡å‹çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (batch_size, tgt_seq_len, tgt_vocab_size)ã€‚\n",
    "        \"\"\"\n",
    "        # ç¼–ç å™¨éƒ¨åˆ†\n",
    "        # 1. å¯¹æºè¯­è¨€è¾“å…¥è¿›è¡Œè¯åµŒå…¥\n",
    "        src_embedded = self.encoder_embedding(src)  # (batch_size, src_seq_len, d_model)\n",
    "        # 2. æ·»åŠ ä½ç½®ç¼–ç \n",
    "        src_embedded = self.positional_encoding(src_embedded)  # (batch_size, src_seq_len, d_model)\n",
    "        # 3. åº”ç”¨ Dropout\n",
    "        src_embedded = self.dropout(src_embedded)  # (batch_size, src_seq_len, d_model)\n",
    "        # 4. é€šè¿‡å¤šä¸ªç¼–ç å™¨å±‚\n",
    "        encoder_output = src_embedded\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, src_mask)  # (batch_size, src_seq_len, d_model)\n",
    "\n",
    "        # è§£ç å™¨éƒ¨åˆ†\n",
    "        # 1. å¯¹ç›®æ ‡è¯­è¨€è¾“å…¥è¿›è¡Œè¯åµŒå…¥\n",
    "        tgt_embedded = self.decoder_embedding(tgt)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # 2. æ·»åŠ ä½ç½®ç¼–ç \n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # 3. åº”ç”¨ Dropout\n",
    "        tgt_embedded = self.dropout(tgt_embedded)  # (batch_size, tgt_seq_len, d_model)\n",
    "        # 4. é€šè¿‡å¤šä¸ªè§£ç å™¨å±‚\n",
    "        decoder_output = tgt_embedded\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output, encoder_output, src_mask, tgt_mask)  # (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "        # è¾“å‡º:å°†è§£ç å™¨çš„è¾“å‡ºæ˜ å°„ä¸ºç›®æ ‡è¯­è¨€è¯æ±‡è¡¨çš„å¤§å°\n",
    "        output = self.linear(decoder_output)  # (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Texts: ['åŒå¿—ä»¬!æˆ‘è¸©ç€åœ°é›·äº†', 'æˆ‘ç°åœ¨å¼€å§‹æ’é›·ã€‚']\n",
      "æºåºåˆ—:\n",
      " tensor([[42016, 78228, 80578,     0, 37046,   164,   116,   102, 84949,   222,\n",
      "         30590, 97565, 35287]], device='cuda:0')\n",
      "ç›®æ ‡åºåˆ—:\n",
      " tensor([[37046, 47551, 19000, 56386, 61056, 97565,  1811,     0,     0,     0,\n",
      "             0,     0,     0]], device='cuda:0')\n",
      "\n",
      "Transformer è¾“å‡ºå½¢çŠ¶:\n",
      " torch.Size([1, 13, 100256])\n"
     ]
    }
   ],
   "source": [
    "# å®ä¾‹åŒ– Transformer æ¨¡å‹\n",
    "transformer = Transformer(vocab_size, vocab_size, d_model, num_heads, num_layers, d_ffn, dropout, max_len).to(device)\n",
    "\n",
    "# å°† Token IDs è§£ç å›æ–‡æœ¬\n",
    "decoded_texts = [tiktok_encoder.decode(token_ids) for token_ids in tokens]\n",
    "print(\"Decoded Texts:\", decoded_texts)\n",
    "\n",
    "src = token_ids[0].unsqueeze(0)  # (batch_size, src_seq_len)\n",
    "tgt = token_ids[1].unsqueeze(0)  # (batch_size, tgt_seq_len)\n",
    "print(\"æºåºåˆ—:\\n\", src)\n",
    "print(\"ç›®æ ‡åºåˆ—:\\n\", tgt)\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "output = transformer(src, tgt)\n",
    "print(\"\\nTransformer è¾“å‡ºå½¢çŠ¶:\\n\", output.shape)  # (batch_size, tgt_seq_len, tgt_vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
